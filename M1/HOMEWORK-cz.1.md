# Zadanie 1

Skonfiguruj sobie dostÄ™p: Anthropic / Gemini / OpenAI.
DostÄ™pne API: python / node.js, w sumie 6 rÃ³Å¼nych kombinacji:
- `M1/external-model-anthropic-node`
- `M1/external-model-anthropic-py`
- `M1/external-model-google-genai-node`
- `M1/external-model-google-genai-py`
- `M1/external-model-openai-node`
- `M1/external-model-openai-py`

Foldery zawierajÄ… README z linkami do zakÅ‚adania kont i kluczy API.
MoÅ¼na zasiliÄ‡ model jednorazowo np. kwotÄ… 5$ i ustawiÄ‡ zmienne typu "maxTokens" na skrajnie niskÄ… wartoÅ›Ä‡ typu 128 - wÃ³wsczas pojedynczy request kosztuje ~0.002$.
Google Gemini daje "hojne" darmowe quota na start.

W wybranym setupie stokenizuj odpowiedzi na wzÃ³r ![tokenized prompts and responses](./tokenized.png), jednoczeÅ›nie uwzglÄ™dniajÄ…c specyfikÄ™ vendorÃ³w:
- **OpenAI**: pakiet `tiktoken` pozwala na **zliczenie tokenÃ³w** + **zwrÃ³cenie listy** konkretnych tokenÃ³w
- **Google genai/Gemini**: pozwala tylko na **zliczenie tokenÃ³w** 
- **Anthropic SDK**: pozwala tylko na **zliczenie tokenÃ³w** 
- **modele otwarte / Hugging Face** (`transformers`/python): pozwala na "prawie wszystko", tj. **zliczenie tokenÃ³w** + **zwrÃ³cenie listy** konkretnych tokenÃ³w

# Zadanie 2

Podepnij MLFlow - i podsÅ‚uchaj, co agent robi z (jakimÅ›) modelem:
- Claude - najÅ‚atwiej (1 komenda CLI)
- Gemini - z poziomu kody python
- Lokalne modele - z poziomu kodu python

zobacz `M1/mlflow/README.md`

# Zadanie 3

TASK: Robimy wÅ‚asny TOKENIZER ("sÅ‚ownik tokenÃ³w"). Folder: `M1/tokenizer`

Korpusy danych treningowych do wyboru:
- `M1/korpus-nkjp` (jest dostÄ™pny w [folderze `M1/korpus-nkjp/output`](./korpus-nkjp/output/), dodatkowo [wiÄ™cej info dot. jak Å›ciÄ…gaÄ‡ tutaj](./korpus-nkjp/README.md))
- `M1/korpus-wolnelektury`
- `M1/korpus-spichlerz` (Bielik Team)
W repo znajdziesz instrukcje dla 3 rÃ³Å¼nych korpusÃ³w danych treningowych oraz bazowy kod pythonowy.
(!) **Korzystaj z plik-utils do zarzÄ…dzania korpusami**: `M1/tokenizer/corpora.py` (importujesz/przefiltrowujesz korpusy).

Zadania:
1. stwÃ³rz wÅ‚asne tokenizery w oparciu o plik `tokenizer-build.py` (obecna wersja dziaÅ‚a ale jest zahardkodowana). Zdynamizuj kod w taki sposÃ³b, aby mÃ³c dynamicznie tworzyÄ‡ tokenizery w oparciu o zadane korpusy tekstowe. StwÃ³rz
  - `tokenizer-pan-tadeusz.json` - tylko w oparciu o Pana Tadeusza ("wolnelektury")
  - `tokenizer-wolnelektury.json` - w oparciu o caÅ‚y korpus "wolnelektury"
  - `tokenizer-nkjp.json` - w oparciu o caÅ‚y korpus "nkjp"
  - `tokenizer-all-corpora.json` - w oparciu o wszystkie korpusy
2. z HuggingFace wybierz LLM i Å›ciÄ…gnij jego tokenizer (byle inny niÅ¼ Mistral-v0.1 - bo to ten sam co Bielik v0.1) i dodaj go do swoich tokenizerÃ³w
3. w nawiÄ…zaniu do sÅ‚awnego badania ;) (https://arxiv.org/pdf/2503.01996) tokenizujemy rÃ³Å¼ne teksty "na krzyÅ¼" rÃ³Å¼nymi tokenizerami
  - teksty:
    - "Pan Tadeusz, KsiÄ™ga 1" ("wolnelektury")
    - "The Pickwick Papers" (mini korpus / projekt gutenberg)
    - "Fryderyk Chopin" (mini korpus / wikipedia)
  - tokenizery - wszystkie dostÄ™pne (3 bielikowe + wybrany z HF + 4 stworzone)
  - zmontuj statystyki, ktÃ³re majÄ… odpowiedzieÄ‡ na pytanie: **DLA KAÅ»DEGO TEKSTU, KTÃ“RY TOKENIZER BYÅ NAJEFEKTYWNIEJSZY POD KÄ„TEM NAJMNIEJSZEJ ILOÅšCI WYNIKOWYCH TOKENÃ“W?**
4. sprÃ³buj osiÄ…gnÄ…Ä‡ taki tokenizer, aby miaÅ‚ jak najdÅ‚uÅ¼sze kawaÅ‚ki sÅ‚Ã³w - to sprawi Å¼e embedding w nastÄ™pnym Ä‡wiczeniu bÄ™dzie mega efektywny
5. sprawdÅº czy dla customowych tokenizerÃ³w zmiana rozmiaru sÅ‚ownika (default: `32k`) robi rÃ³Å¼nicÄ™ na wyniki?

**Finalnie**: porÃ³wnaj tokenizery: swÃ³j + wszystkie bieliki (`M1/tokenizer/tokenizers/*.json`).

**Spodziewane**: tokenizacja Twoim tokenizerem/sÅ‚ownikiem bÄ™dzie najefektywniejsza. W drugiej kolejnoÅ›ci - bielik v3 - w trzeciej kolejnoÅ›ci - bielik v1/v2 (to ten sam tokenizer).

# Zadanie 3 - CIEKAWOSTKA

sneak peak tokenizera (pakiet `tiktoken`) w google colab: https://colab.research.google.com/drive/1cbW-Wnn-_mDUEhC6ptrK9Ltd7nvQu6Mu

# Zadanie 4.1

Folder: `M1/embedding`

Intro:
CBOW (Continuous Bag-of-Words), jest sieciÄ… neuronowÄ…, ktÃ³ra uczy siÄ™ przewidywaÄ‡ sÅ‚owo docelowe (Å›rodkowe) na podstawie jego sÅ‚Ã³w kontekstowych (otaczajÄ…cych), znajdujÄ…cych siÄ™ w okreÅ›lonym oknie.

W pliku `M1/embedding/run-cbow.py` Å‚aduje tokenizer, tokenizuje zadane teksty i buduje w oparciu o nie model embeddingowy typu CBOW (podobieÅ„stwo/czÄ™stotliwoÅ›Ä‡ wystÄ…pieÅ„, co w odpowiednio duÅ¼ej skali zaczyna symulowaÄ‡ podobieÅ„stwo znaczeniowe - dziwnym trafem tak samo jak LLMy :)) 


Cel zadania: znaleÅºÄ‡ takie ustawienia aby sÅ‚owa pokrewne (np. kobieta-dziewczyna, krÃ³l-ksiÄ…Å¼Ä™ byÅ‚y blisko w embeddingu, tj. wartoÅ›Ä‡ moÅ¼liwie bliska 1)

Zadania - w skrÃ³cie:
- rozbij skrypt aby daÅ‚o siÄ™ osobno trenowaÄ‡ i osobno wnioskowaÄ‡ (teraz jest wszystko na raz :)
- wybierz sÅ‚owa/teskt referencyjny (cokolwiek wybierzesz z korpusÃ³w lub wymyÅ›lisz). Punktem odniesienia mogÄ… byÄ‡ sÅ‚owa zahardkodowane w skrypcie
- testuj trenowanie na rÃ³Å¼nych korpusach, roÅ¼nych tokenizerach, roÅ¼nych parametrach


ğŸ”¥ Nie wykonasz tego zadania bez skutecznego wykonania poprzedniego zadania. PorÃ³wnuj pomysÅ‚y na discordzie.
ğŸ”¥ PodpowiedÅº: wsrÃ³d rÃ³Å¼nych ustawieÅ„, jakie bÄ™dziesz zmieniaÄ‡, sprÃ³buj porÃ³wnaÄ‡ takÅ¼e tokenizery - np. bielik-v1 (mistralowy), bielik-v3 oraz TwÃ³j customowy z zadania 3.


PrzykÅ‚adowe oczekiwane wyniki:
```
10 tokenÃ³w najbardziej podobnych do SÅOWA 'wojsko' (uÅ›rednione wektory tokenÃ³w ['wojsko']):
  > Wektor sÅ‚owa (poczÄ…tek): [ 0.8360334  0.0980003 -0.221845  -4.700161   1.4552883]...
  - wojsko: 1.0000
  - miasto: 0.6638
  - wojska: 0.6313
  - Å¼ycie: 0.6036
  - zwyciÄ™stwo: 0.5795
  - rycerstwo: 0.5671
  - paÅ„stwo: 0.5635
  - hetmanÃ³w: 0.5599
  - posiÅ‚ki: 0.5576
  - pospÃ³lstwo: 0.5331

10 tokenÃ³w najbardziej podobnych do SÅOWA 'szlachta' (uÅ›rednione wektory tokenÃ³w ['szlachta']):
  > Wektor sÅ‚owa (poczÄ…tek): [-1.5282736   0.82800084  1.1820822  -2.4249477   1.0725677 ]...
  - szlachta: 1.0000
  - piechota: 0.6810
  - jazda: 0.6259
  - sÅ‚uÅ¼ba: 0.6035
  - starszyzna: 0.6029
  - wojna: 0.5841
  - armia: 0.5670
  - arystokracja: 0.5617
  - Litwa: 0.5551
  - kupa: 0.5538

10 tokenÃ³w najbardziej podobnych do SÅOWA 'choroba' (uÅ›rednione wektory tokenÃ³w ['choroba']):
  > Wektor sÅ‚owa (poczÄ…tek): [ 0.6537147  -0.04082277 -1.689754    0.97554463  0.10579971]...
  - choroba: 1.0000
  - dziewka: 0.6708
  - mÄ™ka: 0.6616
  - natura: 0.6224
  - taka: 0.6102
  - tÄ™sknota: 0.6064
  - osoba: 0.5888
  - jakaÅ›: 0.5863
  - okrutna: 0.5849
  - zmiana: 0.5797

10 tokenÃ³w najbardziej podobnych do SÅOWA 'krÃ³l' (uÅ›rednione wektory tokenÃ³w ['krÃ³l']):
  > Wektor sÅ‚owa (poczÄ…tek): [ 2.4375966  -1.0871804  -1.6425471  -1.6709629  -0.62909025]...
  - krÃ³l: 1.0000
  - ksiÄ…Å¼Ä™: 0.7209
  - Chmielnicki: 0.6851
  - mistrz: 0.6605
  - hetman: 0.6238
  - Karol: 0.6195
  - cezar: 0.6113
  - Jurand: 0.5996
  - Bohun: 0.5919
  - jeneraÅ‚: 0.5895

10 tokenÃ³w najbardziej podobnych do kombinacji tokenÃ³w: ['dziecko', 'kobieta']
  - dziewczyna: 0.6242
  - ona: 0.6124
  - matka: 0.6069
  - dziewka: 0.6049
  - sztuka: 0.5968
  - piÄ™kna: 0.5788
  - mÄ™ka: 0.5731
  - osoba: 0.5724
  - cnota: 0.5683
  - sama: 0.5590
```

krÃ³l - ksiÄ…Å¼e - 0.7209 jest niezÅ‚e (choÄ‡ mogÅ‚oby byÄ‡ lepsze)
['dziecko', 'kobieta'] - dziewczyna: 0.6242 - teÅ¼ niezÅ‚e i teÅ¼ mogÅ‚oby byÄ‡ lepsze.

OczywiÅ›cie Å›miaÅ‚o podmieniaj sÅ‚owa.

# Zadanie 4.2

Szukamy najbardziej podobnych **zdaÅ„**: `M1/embedding/run-doc2vec.py`

Trenujemy nasz wÅ‚asny model embedingowy (dla caÅ‚ych zdaÅ„, nie samych sÅ‚Ã³w).

Zadanie - j/w - zoptymalizuj trening.

Dobierz parametry treningu (analogicznie co wczeÅ›niej) tak, aby optymalnie: zwiÄ™kszyÄ‡ jakoÅ›Ä‡ wychwytywania podobieÅ„stwa i jednoczeÅ›nie wykonywaÄ‡ trening najkrÃ³cej (tj. nie traciÄ‡ czasu i/lub nie "przetrenowaÄ‡" modelu)

W razie potrzeby dostosowuj korpus treningowy.

Zmiana ktÃ³rych parametrÃ³w najbardziej wydÅ‚uÅ¼a trening?

PrzykÅ‚adowy output (caÅ‚kiem sensowny):
```
Zdanie do wnioskowania: "Jestem gÅ‚odny i bardzo chÄ™tnie zjadÅ‚bym coÅ›."
 najbardziej podobnych zdaÅ„ z korpusu:
  - Sim: 0.6533 | Zdanie: Po chwili otworzyÅ‚ je. RzÄ™dzian siedziaÅ‚ ciÄ…gle pod oknem.
  - Sim: 0.6525 | Zdanie: â€“ ja teÅ¼ nie jem
  - Sim: 0.6487 | Zdanie: â€” Nie bÃ³j siÄ™ waÄ‡panna, nie zjem ciÄ™!
  - Sim: 0.6338 | Zdanie: â€“ ja teÅ¼ nie jem znaczy pewno zjem ale nie nie lubiÄ™ ale jest podobno taka zdrowa..
  - Sim: 0.6330 | Zdanie: â€” PrzysÅ‚aÅ‚ im je krÃ³l francuski â€” odrzekÅ‚ opat.
```
PrzykÅ‚adowy output (zdecydowanie bezsensowny):
```
Zdanie do wnioskowania: "Jestem gÅ‚odny i bardzo chÄ™tnie zjadÅ‚bym coÅ›."
5 najbardziej podobnych zdaÅ„ z korpusu:
  - Sim: 0.9350 | Zdanie: â€“ prawdopodobnie. nie nie wiesz po prostu
  - Sim: 0.9336 | Zdanie: â€“ a teraz. ktoÅ› mÄ…drze powiedziaÅ‚ Å¼e..
  - Sim: 0.9332 | Zdanie: DoÅ›Ä‡, gdy niebezpieczeÅ„stwa i Å›miaÅ‚oÅ›Ä‡ przypomnÄ™,
  - Sim: 0.9250 | Zdanie: Jestem twÃ³j stryj; choÄ‡ stary, znam, co serce mÅ‚ode;
  - Sim: 0.9207 | Zdanie: PoleciaÅ‚abym ja
```

# Zadanie 4.3

Plik `M1/embedding/run-sbert.py` - korzystamy z wczeÅ›niej wytrenowanego modelu o ktÃ³ry siÄ™ opieramy. Bierzemy nasze zdania i kodujemy je w "bazie danych" wektorowej (macierz embeddingÃ³w zdaÅ„ z naszego korpusu). I (zwyczajnie) odpytujemy tÄ™ bazÄ™ w odniesieniu do zadanego zdania (ktÃ³re wczeÅ›niej trzeba zaembedowaÄ‡).

Poszukiwanie najbliÅ¼szego wektora w wielowymiarowej przestrzeni.

**Skrypt realizuje swoje zadanie**.

Twoja rola:
- sprÃ³buj znaleÅºÄ‡ alternatywÄ™ dla modelu `SentenceTransformer(MODEL_NAME)` lepiej dostosowanego do jÄ™zyka polskiego
- rozdziel skrypt tak, aby "kodowanie bazy danych" byÅ‚o osobno a sprawdzanie zbieÅºnoÅ›ci osobno.
- odpytaj o zdania wymyÅ›lone oraz takie ktÃ³re bezpoÅ›rednio pochodzÄ… z korpusu treningowego.

# Zadanie 5

Zaimplementuj uproszczonÄ… wersjÄ™ **ATTENTION SCORE MATRIX (S)**
Kod wyjÅ›ciowy: folder `M1/szczypta-machine-learning`.
PosiÅ‚kuj siÄ™ ulubionym coding agent + deep research + discordem ğŸ˜‰

Plik: `M1/szczypta-machine-learning/src/homework.ts`
